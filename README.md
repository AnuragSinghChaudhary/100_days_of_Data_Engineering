# 100_days_of_Data_Engineering
Course work for learning data engineering
Creating a "100 Days of Data Engineering Challenge" where each day involves completing a project or a substantial task is a great way to build a portfolio. Here's a detailed schedule with daily projects:

### Week 1: Core Data Engineering Concepts
1. **Day 1**: **Introduction to Data Engineering**
   - **Project**: Write a blog post or create a presentation on the role of a data engineer, key responsibilities, and an overview of essential tools and technologies.

2. **Day 2**: **Relational Databases and SQL Basics**
   - **Project**: Design a simple relational database (e.g., for a library system). Write SQL queries to create tables, insert data, and perform basic queries.

3. **Day 3**: **Advanced SQL Queries**
   - **Project**: Use a sample dataset (e.g., from Kaggle) and write complex SQL queries involving joins, subqueries, window functions, and aggregations.

4. **Day 4**: **Database Design and Normalization**
   - **Project**: Take an unnormalized dataset and design a normalized database schema for it. Document the process of normalization up to the 3rd Normal Form.

5. **Day 5**: **Data Modeling**
   - **Project**: Create an entity-relationship diagram (ERD) for a hypothetical e-commerce platform, explaining the relationships between entities.

6. **Day 6**: **Data Warehousing Concepts**
   - **Project**: Design a star schema for a retail data warehouse. Populate the schema with sample data and write SQL queries to extract insights.

7. **Day 7**: **Introduction to ETL**
   - **Project**: Build a simple ETL pipeline using Python to extract data from a CSV file, transform it (e.g., cleaning, normalization), and load it into a relational database.

### Week 2: ETL Tools and Batch Processing
8. **Day 8**: **Apache NiFi Basics**
   - **Project**: Install Apache NiFi and create a simple data flow to move data from a local file to a database.

9. **Day 9**: **ETL with Talend**
   - **Project**: Build an ETL pipeline using Talend Open Studio to extract data from a web API, transform it, and load it into a MySQL database.

10. **Day 10**: **AWS Glue**
    - **Project**: Set up an ETL job using AWS Glue to process data stored in an S3 bucket and load it into an AWS Redshift data warehouse.

11. **Day 11**: **Introduction to Apache Hadoop**
    - **Project**: Write a Hadoop MapReduce job to process a large text dataset (e.g., word count program).

12. **Day 12**: **Batch Processing with Apache Pig**
    - **Project**: Use Apache Pig to analyze a dataset stored in HDFS, performing operations like filtering, grouping, and aggregation.

13. **Day 13**: **Data Pipeline with Apache Oozie**
    - **Project**: Create a workflow in Apache Oozie to schedule and manage a Hadoop job.

14. **Day 14**: **Building a Data Pipeline with Python**
    - **Project**: Write a Python script to automate the extraction of data from an API, transform the data, and load it into a PostgreSQL database.

### Week 3: Stream Processing and Big Data Tools
15. **Day 15**: **Introduction to Apache Kafka**
    - **Project**: Set up an Apache Kafka cluster, create a topic, and build a producer-consumer application to process streaming data.

16. **Day 16**: **Stream Processing with Apache Flink**
    - **Project**: Build a real-time stream processing application with Apache Flink to analyze live data from a source like Twitter.

17. **Day 17**: **Apache Spark Basics**
    - **Project**: Write a Spark application to process a large dataset (e.g., a log file), perform transformations, and save the results to HDFS.

18. **Day 18**: **Data Processing with Spark SQL**
    - **Project**: Use Spark SQL to query a large dataset stored in HDFS and generate analytical reports.

19. **Day 19**: **Building a Real-Time Dashboard with Kafka and Spark Streaming**
    - **Project**: Combine Kafka and Spark Streaming to process real-time data and visualize it using a dashboard (e.g., with Grafana).

20. **Day 20**: **Introduction to Apache Cassandra**
    - **Project**: Set up a Cassandra cluster and design a data model for a high-throughput application like an IoT platform.

21. **Day 21**: **Building a Data Lake on AWS**
    - **Project**: Create a simple data lake on AWS using S3, catalog the data with AWS Glue, and query it using AWS Athena.

### Week 4: Cloud Platforms and Data Engineering Best Practices
22. **Day 22**: **Introduction to AWS Redshift**
    - **Project**: Set up an AWS Redshift cluster, load data into it from S3, and write SQL queries to perform analytics.

23. **Day 23**: **Google BigQuery Basics**
    - **Project**: Use Google BigQuery to analyze a large public dataset (e.g., Wikipedia pageviews) and write complex SQL queries.

24. **Day 24**: **Azure Data Factory**
    - **Project**: Create a data pipeline using Azure Data Factory to move data from an on-premise database to Azure SQL Database.

25. **Day 25**: **Introduction to Apache Airflow**
    - **Project**: Install Apache Airflow and create a Directed Acyclic Graph (DAG) to orchestrate an ETL pipeline.

26. **Day 26**: **Building a Data Pipeline with Airflow**
    - **Project**: Create a complete data pipeline using Airflow that extracts data from an API, processes it, and loads it into a data warehouse.

27. **Day 27**: **Data Quality Checks**
    - **Project**: Implement data validation checks in an existing data pipeline using Python or SQL, and set up alerts for data quality issues.

28. **Day 28**: **Data Encryption and Security**
    - **Project**: Secure a data pipeline by encrypting data at rest and in transit, and implement role-based access control.

### Week 5: Advanced Data Engineering Techniques
29. **Day 29**: **Data Governance with Apache Atlas**
    - **Project**: Set up Apache Atlas and create a data catalog for a sample data pipeline, including lineage tracking.

30. **Day 30**: **Implementing CI/CD for Data Pipelines**
    - **Project**: Create a CI/CD pipeline using Jenkins or GitLab CI to automate the deployment of a data pipeline.

31. **Day 31**: **Introduction to Docker for Data Engineering**
    - **Project**: Containerize a data pipeline using Docker and deploy it on a cloud platform.

32. **Day 32**: **Kubernetes for Data Engineering**
    - **Project**: Deploy a containerized data pipeline on a Kubernetes cluster and manage its scaling and orchestration.

33. **Day 33**: **DataOps Practices**
    - **Project**: Implement DataOps principles in a data pipeline, focusing on version control, monitoring, and automation.

34. **Day 34**: **Real-Time Data Processing with Apache Storm**
    - **Project**: Build a real-time data processing application using Apache Storm and Kafka to process live data streams.

35. **Day 35**: **Building a Lambda Architecture**
    - **Project**: Design and implement a Lambda architecture using Hadoop for batch processing and Kafka for real-time data processing.

36. **Day 36**: **Monitoring Data Pipelines with Prometheus and Grafana**
    - **Project**: Set up monitoring for a data pipeline using Prometheus and create a dashboard in Grafana.

37. **Day 37**: **Introduction to Apache Hudi**
    - **Project**: Use Apache Hudi to manage a large dataset in a data lake, supporting incremental updates and data versioning.

38. **Day 38**: **Implementing Delta Lake on Databricks**
    - **Project**: Use Delta Lake on Databricks to create a data lake that supports ACID transactions and scalable data processing.

39. **Day 39**: **Data Pipeline Optimization**
    - **Project**: Optimize an existing data pipeline for performance by improving query efficiency, indexing, and data partitioning.

40. **Day 40**: **Introduction to Apache Druid**
    - **Project**: Set up an Apache Druid cluster and load a dataset into it to perform real-time analytics.

### Week 6: Real-World Projects and Case Studies
41. **Day 41**: **Building a Data Warehouse for an E-commerce Company**
    - **Project**: Design and implement a data warehouse for an e-commerce company, including ETL pipelines to populate it with data.

42. **Day 42**: **Building a Recommendation Engine**
    - **Project**: Build a data pipeline that processes user behavior data to create input for a recommendation engine.

43. **Day 43**: **IoT Data Processing Pipeline**
    - **Project**: Design and implement a pipeline to process IoT sensor data in real-time using Kafka and Spark Streaming.

44. **Day 44**: **Fraud Detection Pipeline**
    - **Project**: Create a data pipeline that processes financial transactions and identifies potential fraud in real-time.

45. **Day 45**: **Log Analytics Pipeline**
    - **Project**: Build a data pipeline that ing

ests, processes, and analyzes log data from multiple sources using ELK Stack (Elasticsearch, Logstash, Kibana).

46. **Day 46**: **Customer Segmentation Pipeline**
    - **Project**: Create a data pipeline to process customer data and perform segmentation analysis, generating insights for targeted marketing.

47. **Day 47**: **Data Pipeline for Social Media Analytics**
    - **Project**: Build a pipeline that collects, processes, and analyzes social media data to generate sentiment analysis reports.

48. **Day 48**: **Real-Time Event Processing for Gaming**
    - **Project**: Design and implement a real-time event processing pipeline for a gaming platform to track user activity and generate in-game recommendations.

49. **Day 49**: **Healthcare Data Processing Pipeline**
    - **Project**: Build a pipeline that processes healthcare data, including patient records, and performs analysis for predictive modeling.

50. **Day 50**: **Building a Financial Data Warehouse**
    - **Project**: Design and implement a data warehouse for a financial institution, focusing on ETL processes to handle high-volume transaction data.

### Week 7-10: Building and Documenting Portfolio Projects
51. **Day 51**: **Building a Portfolio Website**
    - **Project**: Create a personal website to showcase your data engineering projects, including detailed descriptions and code samples.

52. **Day 52**: **Project Documentation**
    - **Project**: Document one of your major projects, including the problem statement, solution architecture, implementation details, and results.

53. **Day 53**: **Blog Post: My Data Engineering Journey**
    - **Project**: Write a blog post or article summarizing your journey through the 100 Days of Data Engineering Challenge, including lessons learned and key takeaways.

54. **Day 54**: **GitHub Repository Management**
    - **Project**: Organize your GitHub repositories, ensuring each project is well-documented and has a clear README file.

55. **Day 55**: **Polishing Your Resume**
    - **Project**: Update your resume to highlight your data engineering projects, including links to your GitHub and portfolio website.

56. **Day 56**: **Creating a LinkedIn Profile**
    - **Project**: Update your LinkedIn profile with your latest skills, projects, and achievements from the challenge.

57. **Day 57**: **Applying for Jobs**
    - **Project**: Identify and apply for at least 5 data engineering jobs in Paris that align with your skills and experience.

58. **Day 58**: **Mock Technical Interview**
    - **Project**: Conduct a mock technical interview focused on data engineering concepts, including coding exercises and problem-solving.

59. **Day 59**: **Mock Behavioral Interview**
    - **Project**: Practice answering common behavioral interview questions, focusing on how your projects demonstrate key competencies.

60. **Day 60**: **Networking on LinkedIn**
    - **Project**: Connect with professionals in the data engineering field, share your portfolio, and seek advice or mentorship.

### Week 11-14: Advanced Projects and Networking
61. **Day 61**: **Contributing to Open Source**
    - **Project**: Identify an open-source data engineering project on GitHub and contribute by fixing a bug or adding a feature.

62. **Day 62**: **Building a Data Engineering Toolkit**
    - **Project**: Compile a list of your favorite tools, libraries, and frameworks used during the challenge, and create a blog post or GitHub repository to share with others.

63. **Day 63**: **Building a Real-Time Data Lake**
    - **Project**: Create a data lake architecture that supports both batch and real-time data processing using a combination of tools like Kafka, Spark, and S3.

64. **Day 64**: **Data Engineering in the Cloud**
    - **Project**: Migrate an on-premise data pipeline to a cloud platform like AWS, Azure, or GCP, and document the process.

65. **Day 65**: **Creating a Data Engineering Course**
    - **Project**: Design a mini-course or tutorial series based on one of your projects, and publish it online (e.g., on YouTube or a personal blog).

66. **Day 66**: **Participating in a Hackathon**
    - **Project**: Join a data engineering or data science hackathon, and document your experience and solution.

67. **Day 67**: **Creating a Webinar on Data Engineering**
    - **Project**: Host a webinar or live session on a topic you’ve mastered during the challenge, and share your knowledge with others.

68. **Day 68**: **Advanced Data Pipeline with Serverless Architecture**
    - **Project**: Build a data pipeline using serverless technologies like AWS Lambda, DynamoDB, and S3, focusing on scalability and cost-effectiveness.

69. **Day 69**: **Real-Time Fraud Detection System**
    - **Project**: Develop a real-time fraud detection system for financial transactions using Kafka, Flink, and a machine learning model.

70. **Day 70**: **Optimizing a Data Lake for Performance**
    - **Project**: Tune a data lake's performance by optimizing data partitioning, indexing, and query strategies.

### Week 13-14: Final Projects, Networking, and Job Search
71. **Day 71**: **Final Portfolio Project**
    - **Project**: Select a significant project idea and execute it from start to finish, demonstrating all aspects of data engineering.

72. **Day 72**: **Documentation of Final Project**
    - **Project**: Thoroughly document your final project, including the problem statement, solution design, implementation, and results.

73. **Day 73**: **Video Presentation of Final Project**
    - **Project**: Record a video presentation explaining your final project, showcasing your skills and thought process.

74. **Day 74**: **Creating a Case Study**
    - **Project**: Write a detailed case study on one of your projects, emphasizing the business problem, technical challenges, and solution impact.

75. **Day 75**: **Networking Event or Meetup**
    - **Project**: Attend a networking event or meetup focused on data engineering or related fields to meet professionals and expand your network.

76. **Day 76**: **Job Application Sprint**
    - **Project**: Apply for at least 10 data engineering positions, tailoring your resume and cover letter to each one.

77. **Day 77**: **Final Resume Review**
    - **Project**: Review your resume with a mentor or professional, making final adjustments to optimize it for job applications.

78. **Day 78**: **LinkedIn Engagement**
    - **Project**: Write and share a LinkedIn post about your 100 Days of Data Engineering Challenge, including insights and a link to your portfolio.

79. **Day 79**: **Interview Preparation**
    - **Project**: Prepare for upcoming interviews by reviewing key data engineering concepts, coding challenges, and case studies.

80. **Day 80**: **Mock Interview with a Peer**
    - **Project**: Conduct a mock interview with a peer or mentor, focusing on both technical and behavioral questions.

81. **Day 81**: **Advanced Cloud Data Engineering**
    - **Project**: Implement a multi-cloud data pipeline using services from AWS, Azure, and GCP, and document the benefits and challenges.

82. **Day 82**: **Data Pipeline for Machine Learning**
    - **Project**: Create a data pipeline that prepares and serves data for a machine learning model, including data preprocessing and feature engineering.

83. **Day 83**: **Data Governance Framework**
    - **Project**: Develop a data governance framework for a fictional organization, focusing on data quality, security, and compliance.

84. **Day 84**: **Real-Time Analytics with Apache Druid**
    - **Project**: Set up Apache Druid and create a real-time analytics dashboard for a large dataset.

85. **Day 85**: **Building a Scalable ETL Pipeline**
    - **Project**: Design and implement a highly scalable ETL pipeline using a combination of cloud services and big data tools.

86. **Day 86**: **Final Portfolio Review**
    - **Project**: Review and finalize your portfolio, ensuring all projects are well-documented and presented.

87. **Day 87**: **Final Networking Push**
    - **Project**: Reach out to professionals in your network for advice, job referrals, or informational interviews.

88. **Day 88**: **Application Follow-Up**
    - **Project**: Follow up on all job applications, checking in with recruiters or hiring managers.

89. **Day 89**: **Creating a Personal Brand**
    - **Project**: Develop your personal brand as a data engineer by creating content, engaging on social media, and establishing yourself as an expert in the field.

90. **Day 90**: **Preparation for Job Offers**
    - **Project**: Prepare for salary negotiations and job offer evaluations, ensuring you understand the market and your value.

### Final 10 Days: Reflect, Refine, and Continue Learning
91. **Day 91**: **Reflection on the Challenge**
    - **Project**: Write a reflective piece on what you learned during the 100 Days of Data Engineering Challenge, highlighting key achievements and areas for growth.

92. **Day 92**: **Mentoring Others**
    - **Project**: Offer to mentor someone who is starting in data engineering, sharing your experience and guiding them through their learning journey.

93. **Day 93**: **Exploring New Tools and Technologies**


    - **Project**: Spend a day exploring new tools or technologies in data engineering that you haven't yet used, experimenting with a small project.

94. **Day 94**: **Deep Dive into a Specific Topic**
    - **Project**: Choose a specific area of data engineering that interests you (e.g., stream processing, data lakes) and do a deep dive, creating a mini-project to apply what you learn.

95. **Day 95**: **Building an Automation Tool**
    - **Project**: Create a small tool or script to automate a repetitive task in data engineering, improving efficiency in your workflow.

96. **Day 96**: **Final Networking and Follow-Ups**
    - **Project**: Revisit your network, send follow-up messages, and attend any final networking events before completing the challenge.

97. **Day 97**: **Preparing for Your First Job**
    - **Project**: Prepare for your first job in data engineering by reviewing common industry practices, tools, and the typical onboarding process.

98. **Day 98**: **Continuous Learning Plan**
    - **Project**: Create a plan for continuous learning beyond the 100 days, identifying areas you want to explore further and setting goals for the next six months.

99. **Day 99**: **Sharing Your Journey**
    - **Project**: Share your entire 100-day journey on a platform like LinkedIn, Medium, or YouTube, providing insights and advice for others.

100. **Day 100**: **Celebrating Your Achievement**
    - **Project**: Take time to celebrate completing the 100 Days of Data Engineering Challenge, reflect on your progress, and plan your next steps in your career.

This schedule should give you a comprehensive learning experience and a strong portfolio, making you well-prepared for a career in data engineering.
